# Functions for data wrangling
This is a markdown rendering of the `wrangling_utils` module used in the notebooks. It is provided here for user reference. The code can be viewed and downloaded from the github repository.


```
""" wrangling_utils.py

    Utility functions for simple data wrangling, including restricting data to certain regions and interpolating data

"""

import xarray as xr 
import pandas as pd 
import numpy as np 
import numpy.ma as ma 
from scipy.interpolate import griddata
from scipy.spatial import KDTree
from astropy.convolution import convolve
from astropy.convolution import Gaussian2DKernel
```


```
def restrictRegionally(dataset, regionKeyList): 
    """Restrict dataset to input regions.
    
    Args: 
        dataset (xr Dataset): dataset generated by Load_IS2 notebook
        regionKeyList (list): list of region keys to restrict data to 
        
    Returns: 
        regionalDataset (xr Dataset): dataset with restricted data to input regions
    """
    
    # If the user imputs a DataArray, convert to Dataset 
    if type(dataset) == xr.DataArray: 
        dataset = dataset.to_dataset()
    
    def checkKeys(regionKeyList, regionTbl): 
        """Check that regionKeyList was defined correctly

        Raises: 
            ValueError if regionKeyList was not defined correctly 
            warning if all data was removed from the dataset
        """
        if type(regionKeyList) != list: #raise a ValueError if regionKeyList is not a list 
            raise ValueError('regionKeyList needs to be a list. \nFor example, if you want to restrict data to the Beaufort Sea, define regionKeyList = [13]')
        for key in regionKeyList: 
            if key not in list(regionTbl['key']): 
                raise ValueError('Region key ' + str(key) + ' does not exist in region mask. \n Redefine regionKeyList with key numbers from table')
        if len(regionKeyList) == 0: 
            warnings.warn('You removed all the data from the dataset. Are you sure you wanted to do this? \n If not, make sure the list regionKeyList is not empty and try again. \n If you intended to keep data from all regions, set regionKeyList = list(tbl[\"key\"])')
 
    #create a table of keys and labels
    regionMask = dataset.region_mask.attrs
    regionTbl = pd.DataFrame({'key': regionMask['keys'], 'label': regionMask['labels']})
    
    #call function to check if regionKeyList was defined correctly
    checkKeys(regionKeyList, regionTbl)
    
    #filter elements from the ice thickness DataArray where the region is the desired region
    keysToRemove = [key for key in list(regionTbl['key']) if key not in regionKeyList]
    regionalDataset = dataset.copy()
    for var in dataset.data_vars: 
        regionalVar = regionalDataset[var]
        for key in keysToRemove: 
            try:
                regionalVar = regionalVar.where(regionalVar['region_mask'] != key)
            except: 
                pass
        regionalDataset[var] = regionalVar
    
    #add new attributes describing changes made to the dataset
    labels = [regionTbl[regionTbl['key'] == key]['label'].item() for key in regionKeyList]
    if len(labels) < len(regionTbl['key']): 
        if set(regionKeyList) == set([10,11,12,13,15]): #convert to sets so unordered lists are compared
            regionalDataset.attrs['regions with data'] = 'Inner Arctic'
        else:    
            regionalDataset.attrs['regions with data'] = ('%s' % ', '.join(map(str, labels)))
        print('Regions selected: ' + regionalDataset.attrs['regions with data'])
    else: 
        regionalDataset.attrs['regions with data'] = 'All'
        print('Regions selected: All \nNo regions will be removed')
    
    return regionalDataset
```


```
def is2_interp2d(is2_ds, cdr_da, method="linear", interp_var=["ice_thickness","freeboard"], suffix="_smoothed", polehole_lat=88.25, x_stddev=0.5, distance_m=50000): 
    """ Perform 2D interpolation over geographic coordinates for all ICESat-2 sea ice variables with geographic coordinates in xr.Dataset
    As of 06/02/2021, xarray does not have a 2D interpolation function so this function is built on scipy.interpolate.griddata (https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.griddata.html)
    
    Args: 
        is2_ds (xr.Dataset): ICESat-2 dataset containing variables to interpolate
        cdr_da (xr.DataArray): NSIDC sea ice concentration. Must contain the same time variable as is2_ds
        method (str,optional): interpolation method (default to "linear", choose from {‘linear’, ‘nearest’, ‘cubic’})
        interp_var (srt or list, optional): variables to interpolate (default to ["ice_thickness","freeboard"])
        suffix (str, optional): suffix to add to end of data variable name to indicate that the variable has been interpolated (default to "smoothed")
        polehole_lat (float, optional): latitude defining pole hole (default to 88.25); set to 90 to keep pole hole 
        x_stddev (float, optional): standard deviation to use for gauassian smoothing (default to 0.5)
        distance_m (float, optional): distance in meters to use for KD tree (default to 50000 meters (50km))
        
    Returns: 
        ds_interp_sorted (xr.Dataset): dataset with interpolated variables, in alphabetical order 
    
    """
    if type(interp_var) == str: 
        interp_var = [interp_var]

    # Get geographic coordinates
    xgrid = is2_ds['xgrid'].values
    ygrid = is2_ds['ygrid'].values
    lats = is2_ds['latitude'].values

    # Loop through variables and timesteps and interpolate 
    ds_interp = is2_ds.copy()
    for var in interp_var: 
        var_interp_list = []
        try: 
            da_var = is2_ds[var].copy()
        except: 
            print(var+" not found in data variables.\nskipping")
            pass 
        for timestep in is2_ds.time.values:
            # Convert xr.DataArray --> numpy array 
            is2_np = da_var.sel(time=timestep).values 
            cdr_np = cdr_da.sel(time=timestep).values
           
            # Interpolate
            is2_np[np.where(cdr_np<0.15)] = 0 # Set 15% conc or less to 0 thickness
            np_interpolated = griddata((xgrid[(np.isfinite(is2_np))], 
                                        ygrid[(np.isfinite(is2_np))]), 
                                        is2_np[(np.isfinite(is2_np))].flatten(),
                                        (xgrid, ygrid), 
                                        fill_value=np.nan,
                                        method=method)
            
            # Gaussian smoothing
            kernel = Gaussian2DKernel(x_stddev=x_stddev)
            np_interpolated_gauss = convolve(np_interpolated, kernel)

            # KDTree: remove distances > 50km 
            xS = xgrid[np.where((np.isfinite(is2_np)))]
            yS = ygrid[np.where((np.isfinite(is2_np)))]
            grid_points = np.c_[xgrid.ravel(), ygrid.ravel()]
            tree = KDTree(np.c_[xS, yS])
            dist, _ = tree.query(grid_points, k=1)
            dist = dist.reshape(xgrid.shape)
            is2_kdtree = np_interpolated_gauss.copy()
            is2_kdtree[np.where((dist>distance_m)&(lats<polehole_lat))] = np.nan # Set any values > distance_m from the next grid cell to 0. Ignore pole hole
            
            # Remove where cdr data meets certain conditions 
            is2_kdtree[~(np.isfinite(cdr_np))] = np.nan # Remove thickness data where cdr data is nan 
            is2_kdtree[np.where(cdr_np<0.5)] = np.nan # Remove thickness data where cdr data < 50% concentration

            # Convert numpy array --> xr.DataArray
            da_interp = xr.DataArray(data=is2_kdtree, 
                                     dims=da_var.sel(time=timestep).dims, 
                                     coords=da_var.sel(time=timestep).coords,
                                     attrs={**da_var.sel(time=timestep).attrs, 'interpolation_description': 'data has been smoothed using the following method: '+method+' interpolation, gaussian smoothing using a standard deviation of '+str(x_stddev)+', and using a KDTree to remove cells > '+str(distance_m)+' meters from nearest cell with data'},
                                     name=da_var.name)
            da_interp = da_interp.expand_dims("time") # Add time as a dimension. Allows for merging DataArrays 
            var_interp_list.append(da_interp)
        
        var_interp = xr.merge(var_interp_list) # Merge all timesteps together 
        ds_interp[var+suffix] = var_interp[var] # Add interpolated variables as data variable original dataset. If suffix = "", the interpolated variable will replace the original variable 

    ds_interp_sorted = ds_interp[sorted(ds_interp.data_vars)] # Sort data variables by alphabetical order
    return ds_interp_sorted  
```


```
def create_empty_xr_ds(xr_ds, start_date, end_date, freq="MS"):
    """ Create an empty xarray dataset for the date range defined by start date --> end date
    
        Args: 
            xr_ds (xr.Dataset, xr.DataArray): dataset to model dimensions off of 
            start_date (str): date for which to start time dimension (i.e "2021-01")
            end_date (str): date for which to end time dimension (default to "2021-04-01")
            freq (str, optional): freqency to do sampling (default to month start, "MS")
        
        Returns: 
            empty_xr_ds (xr.Dataset): empty dataset with time dimension set to the datetime range define by start date -> end date 
    
    """
    if type(xr_ds) == xr.DataArray: 
        xr_ds = xr_ds.to_dataset()

    months = pd.date_range(start=start_date, end=end_date, freq=freq) # date range with month start frequency 
    xr_ds_nan = xr.Dataset(data_vars=xr_ds.where(np.isnan(xr_ds), np.nan).data_vars) # xr_ds, but with all variables set to nan 
    xr_ds_nan_one_timestep = xr_ds_nan.isel(time=0) # Just grab one timestep
    empty_xr_ds = xr.concat([xr_ds_nan_one_timestep]*len(months), dim="time") # Make a xr.Dataset with desired number of empty months
    empty_xr_ds["time"] = months # Reassign time dimension to desired date range 
    return empty_xr_ds
```
